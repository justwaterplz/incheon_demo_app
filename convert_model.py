import torch
import torch.nn as nn
from torch.utils.mobile_optimizer import optimize_for_mobile
import os

# Emergency Detection Model Architecture
# ì´ í´ë˜ìŠ¤ëŠ” ì›ë³¸ GitHub í”„ë¡œì íŠ¸ì˜ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤
class EmergencyDetectionModel(nn.Module):
    """
    Androidìš© ì‘ê¸‰ìƒí™© íƒì§€ ëª¨ë¸
    ì›ë³¸ GitHub í”„ë¡œì íŠ¸ (incheon_publicdata)ì˜ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.
    """
    def __init__(self, num_classes=2):
        super(EmergencyDetectionModel, self).__init__()
        
        # ê¸°ë³¸ì ì¸ CNN êµ¬ì¡° (ì‹¤ì œ ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
        self.features = nn.Sequential(
            # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
            nn.Conv2d(3, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
            
            # ì„¸ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        
        # ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((7, 7)),
            nn.Flatten(),
            nn.Linear(256 * 7 * 7, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

def convert_model_to_mobile(model_path, output_path):
    """
    PyTorch ëª¨ë¸ì„ Androidìš© TorchScript Lite í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        model_path (str): ì›ë³¸ .pt íŒŒì¼ ê²½ë¡œ
        output_path (str): ë³€í™˜ëœ .ptl íŒŒì¼ì´ ì €ì¥ë  ê²½ë¡œ
    """
    try:
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        model = EmergencyDetectionModel(num_classes=2)
        
        # ì €ì¥ëœ íŒŒë¼ë¯¸í„° ë¡œë“œ
        if os.path.exists(model_path):
            # state_dict ë¡œë“œ (íŒŒë¼ë¯¸í„°ë§Œ ìˆëŠ” ê²½ìš°)
            try:
                state_dict = torch.load(model_path, map_location='cpu')
                if isinstance(state_dict, dict):
                    # state_dictë§Œ ìˆëŠ” ê²½ìš°
                    model.load_state_dict(state_dict)
                else:
                    # ì „ì²´ ëª¨ë¸ì´ ì €ì¥ëœ ê²½ìš°
                    model = state_dict
                print("âœ“ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ íŒŒë¼ë¯¸í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ê¸°ë³¸ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")
        else:
            print(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            print("ê¸°ë³¸ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        
        # ì˜ˆì œ ì…ë ¥ í…ì„œ ìƒì„± (ë°°ì¹˜ í¬ê¸° 1, 3ì±„ë„, 224x224)
        example_input = torch.randn(1, 3, 224, 224)
        
        print("TorchScript ë³€í™˜ ì¤‘...")
        
        # TorchScriptë¡œ ë³€í™˜ (trace ë°©ì‹ ì‚¬ìš©)
        traced_model = torch.jit.trace(model, example_input)
        
        print("ëª¨ë°”ì¼ ìµœì í™” ì¤‘...")
        
        # ëª¨ë°”ì¼ìš© ìµœì í™”
        optimized_model = optimize_for_mobile(traced_model)
        
        # Lite Interpreterìš©ìœ¼ë¡œ ì €ì¥
        optimized_model._save_for_lite_interpreter(output_path)
        
        print(f"âœ“ ë³€í™˜ ì™„ë£Œ: {output_path}")
        
        # ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print("ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        test_lite_model(output_path)
        
        return True
        
    except Exception as e:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return False

def test_lite_model(lite_model_path):
    """
    ë³€í™˜ëœ Lite ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
    """
    try:
        # Lite ëª¨ë¸ ë¡œë“œ
        lite_model = torch.jit.load(lite_model_path)
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        test_input = torch.randn(1, 3, 224, 224)
        
        # ì¶”ë¡  ì‹¤í–‰
        with torch.no_grad():
            output = lite_model(test_input)
        
        print(f"âœ“ í…ŒìŠ¤íŠ¸ ì„±ê³µ - ì¶œë ¥ í˜•íƒœ: {output.shape}")
        print(f"  ì¶œë ¥ ê°’ (ë¡œì§“): {output.numpy().flatten()}")
        
        # í™•ë¥ ë¡œ ë³€í™˜
        probabilities = torch.softmax(output, dim=1)
        print(f"  í™•ë¥ : {probabilities.numpy().flatten()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    model_path = "model.pt"  # ì›ë³¸ ëª¨ë¸ íŒŒì¼
    output_path = "app/src/main/assets/emergency_model.ptl"  # ë³€í™˜ëœ ëª¨ë¸ì´ ì €ì¥ë  ê²½ë¡œ
    
    print("=== PyTorch ëª¨ë¸ì„ Androidìš© TorchScript Liteë¡œ ë³€í™˜ ===")
    print(f"ì…ë ¥ ëª¨ë¸: {model_path}")
    print(f"ì¶œë ¥ ê²½ë¡œ: {output_path}")
    
    # assets ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # ë³€í™˜ ì‹¤í–‰
    success = convert_model_to_mobile(model_path, output_path)
    
    if success:
        print("\nğŸ‰ ëª¨ë¸ ë³€í™˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"Android ì•±ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸: {output_path}")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. Android í”„ë¡œì íŠ¸ë¥¼ ë¹Œë“œí•˜ì„¸ìš”")
        print("2. ì‹¤ì œ ë””ë°”ì´ìŠ¤ì—ì„œ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”")
    else:
        print("\nâŒ ëª¨ë¸ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("ì›ë³¸ GitHub í”„ë¡œì íŠ¸ì—ì„œ ëª¨ë¸ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê³  EmergencyDetectionModel í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main() 